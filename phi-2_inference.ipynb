{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig)\n",
    "import threading\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_identifier = \"D:/chat model/model\"\n",
    "enable_4bit = True\n",
    "compute_dtype_bnb = \"float16\"\n",
    "quant_type_bnb = \"nf4\"\n",
    "double_quant_flag = False\n",
    "device_assignment = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_computation = getattr(torch, compute_dtype_bnb)\n",
    "\n",
    "# BitsAndBytes configuration for model quantization\n",
    "bnb_setup = BitsAndBytesConfig(load_in_4bit=enable_4bit,\n",
    "                               bnb_4bit_quant_type=quant_type_bnb,\n",
    "                               bnb_4bit_use_double_quant=double_quant_flag,\n",
    "                               bnb_4bit_compute_dtype=dtype_computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e97eed8bbf343c9829855bc23ff2094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(model_identifier, quantization_config=bnb_setup, device_map=device_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapters_path = 'D:/chat model/adapter'\n",
    "model = PeftModel.from_pretrained(llama_model, adapters_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = AutoTokenizer.from_pretrained(model_identifier, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopAtPunctuation(StoppingCriteria):\n",
    "    def __init__(self, stop_token_ids):\n",
    "        self.stop_token_ids = stop_token_ids\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        last_token_id = input_ids[0, -1].item()\n",
    "        return last_token_id in self.stop_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, model, tokenizer, max_length=1000):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    stop_tokens = [tokenizer.encode(\"#\")[0], tokenizer.encode(\"!\")[0], tokenizer.encode(\"?\")[0]]\n",
    "    stop_criteria = StoppingCriteriaList([StopAtPunctuation(stop_tokens)])\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_length,max_new_tokens=200, pad_token_id=tokenizer.eos_token_id,stopping_criteria=stop_criteria )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    user_prompt = request.json.get('prompt')\n",
    "    generated_text = generate_text(f\"{user_prompt}\", model, llama_tokenizer)\n",
    "    return jsonify(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/submit_prompt', methods=['POST'])\n",
    "def submit_prompt():\n",
    "    user_prompt = request.form['prompt']\n",
    "    url = 'http://192.168.29.193:5000/predict'  # Ensure this points to the correct LLM API\n",
    "    try:\n",
    "        response = requests.post(url, json={'prompt': user_prompt})\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            generated_text = response.text.replace('\\\\n', '<br>')\n",
    "        else:\n",
    "            generated_text = f\"Error: Received status code {response.status_code}\"\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        generated_text = f\"Error connecting to the LLM API: {e}\"\n",
    "\n",
    "    return render_template('index.html', prompt=user_prompt, result=generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_flask():\n",
    "    app.run(host='0.0.0.0', port=5000, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.29.193:5000\n",
      "Press CTRL+C to quit\n",
      "192.168.29.13 - - [24/Sep/2024 21:58:37] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "192.168.29.13 - - [24/Sep/2024 21:58:44] \"GET / HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "192.168.29.193 - - [24/Sep/2024 21:59:10] \"POST /predict HTTP/1.1\" 200 -\n",
      "192.168.29.13 - - [24/Sep/2024 21:59:10] \"POST /submit_prompt HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "192.168.29.193 - - [24/Sep/2024 22:01:11] \"POST /predict HTTP/1.1\" 200 -\n",
      "192.168.29.13 - - [24/Sep/2024 22:01:11] \"POST /submit_prompt HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "192.168.29.193 - - [24/Sep/2024 22:02:03] \"POST /predict HTTP/1.1\" 200 -\n",
      "192.168.29.13 - - [24/Sep/2024 22:02:03] \"POST /submit_prompt HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "192.168.29.193 - - [24/Sep/2024 22:02:52] \"POST /predict HTTP/1.1\" 200 -\n",
      "192.168.29.13 - - [24/Sep/2024 22:02:52] \"POST /submit_prompt HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "192.168.29.193 - - [24/Sep/2024 22:04:37] \"POST /predict HTTP/1.1\" 200 -\n",
      "192.168.29.13 - - [24/Sep/2024 22:04:37] \"POST /submit_prompt HTTP/1.1\" 200 -\n",
      "Both `max_new_tokens` (=200) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "192.168.29.193 - - [24/Sep/2024 22:05:10] \"POST /predict HTTP/1.1\" 200 -\n",
      "192.168.29.13 - - [24/Sep/2024 22:05:10] \"POST /submit_prompt HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "flask_thread = threading.Thread(target=run_flask)\n",
    "flask_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flask_thread.join(timeout=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('llmenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96893c8607dc97b003d35d006dea5dda501bef0f0b6219a24909f378e5a45af9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
